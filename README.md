# Learning Behavior of Other Agents through Observation

## Overview
This repository contains the implementation of a research paper focused on developing an agent capable of learning the behavior of other agents by observing their actions and states, without access to their reward functions. The objective is for our agent to identify expert agents that share the same reward function and to extract their knowledge to enhance its own performance.

## Key Features

### Behavior Cloning
The agent models the policies of social agents using behavior cloning, which can be applied in two scenarios:
1. **Offline**: Utilizing a large dataset of state-action (`<s, a>`) tuples to create precise policy models of other agents.
2. **Online**: Concurrently generating the dataset through the agent's interactions within the environment.

### Scoring System
A scoring system is designed to evaluate agents (including our agent) based on their episodic rewards. The scores are processed through a SoftMax function to generate probabilities. At the beginning of each episode, an agent is selected from this distribution to interact with the environment and obtain rewards. The score is the average of an agent's previous episodic rewards, which is considered the reward of the entire system from an external perspective.

### Expert Agent Selection
Over time, the scoring system tends to favor expert agents, leading to:
- High rewards for the system consistently.
- Creation of excellent expert trajectories that enhance our agent's learning process.

## Advantages
1. **Identification and Exploitation of Expert Agents**: By identifying and exploiting expert agents, the system consistently achieves high rewards, benefiting our agent and the overall system.
2. **Enhanced Learning through Expert Trajectories**: Crafting a high-quality replay buffer of expert trajectories boosts the learning efficiency of our agent.

## Implementation Details
- **Base RL Algorithm**: Soft Actor-Critic (SAC) is used as the base reinforcement learning algorithm for our agent.
- **Offline Learning Enhancement**: Conservative Q-Learning is incorporated as an add-on to improve the offline learning process, especially when the replay buffer contains trajectories not generated by our agent.

## References
